{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, time, tensorflow_addons as tfa, tensorflow as tf, tensorflow.keras as keras, os\n",
    "import scipy\n",
    "from keras.layers import Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import torch\n",
    "import wandb\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jcecilya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jcecilya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jcecilya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "!pip install -U sentence_transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading input variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/train.csv')\n",
    "df_test= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/test.csv')\n",
    "titles= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/titles.csv')\n",
    "train_df=df_train.merge(titles, left_on='context', right_on='code', how='left')\n",
    "test_df=df_test.merge(titles, left_on='context', right_on='code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>group</th>\n",
       "      <th>main_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score code  \\\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  A47   \n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  A47   \n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25  A47   \n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  A47   \n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  A47   \n",
       "\n",
       "                                               title section  class subclass  \\\n",
       "0  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "1  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "2  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "3  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "4  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "\n",
       "   group  main_group  \n",
       "0    NaN         NaN  \n",
       "1    NaN         NaN  \n",
       "2    NaN         NaN  \n",
       "3    NaN         NaN  \n",
       "4    NaN         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DATA PREPARATION***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing input text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_stopwords(sent):\n",
    "    new_sent=[]\n",
    "    filtered_words = \" \".join([word.lower() for word in sent.split(\" \") if word not in stopwords.words('english')])\n",
    "    x1=\"\".join([word for word in filtered_words if word not in string.punctuation])\n",
    "    for x in x1:\n",
    "        new_sent.append(lemmatizer.lemmatize(x, pos =\"v\"))\n",
    "    return \"\".join(new_sent) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text_a\"] = (train_df['anchor']+' '+train_df['title']+' '+train_df['target']).apply(lambda x: lemma_stopwords(x))\n",
    "sentence_pairs = train_df[\"text_a\"].values.astype(str)\n",
    "y = train_df['score'].values.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 1: Using PatentSBERTa sentence transformer and data modelling using XGB algorithm***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Huggingface's pretrained PatentSBERTa sentence transformer**\n",
    "\n",
    "It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "https://huggingface.co/AI-Growth-Lab/PatentSBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strans = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
    "X = strans.encode(sentence_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download and load transformers for resuability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sberta_trans.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentence_pairs, 'embeddings': X}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "import pickle\n",
    "#Load sentences & embeddings from disc\n",
    "with open('sberta_trans.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    X = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "#Load sentences & embeddings from disc\n",
    "with open('sberta_trans.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    X = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split input for testing. Data Modelling with hyper parameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)\n",
    "model = xgb.XGBRegressor(seed=20)\n",
    "params= {\n",
    "    \"learning_rate\":[0.15], \n",
    "    \"max_depth\":[15,25]\n",
    "}\n",
    "\n",
    "model = GridSearchCV(model,params, cv=5, n_jobs= -1, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "z=model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(z, open('xgb.pkl', 'wb'))\n",
    "with open('xgb.pkl' , 'rb') as f:\n",
    "    lr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05023928121731964"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = lr.predict(X_test)\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"text_a\"] = (test_df['anchor']+' '+test_df['title']+' '+test_df['target']).apply(lambda x: lemma_stopwords(x))\n",
    "sentence_pairs_test = test_df[\"text_a\"].values.astype(str)\n",
    "test_encoded = strans.encode(sentence_pairs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.584368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.501242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.497741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.254475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.467312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.498677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.002952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.252589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>0.645575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.248490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.253832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.748997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.996195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b9fdc772bb8fd61c</td>\n",
       "      <td>0.752162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7aa5908a77a7ec24</td>\n",
       "      <td>0.250842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d19ef3979396d47e</td>\n",
       "      <td>0.252857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fd83613b7843f5e1</td>\n",
       "      <td>0.001611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2a619016908bfa45</td>\n",
       "      <td>0.547213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>733979d75f59770d</td>\n",
       "      <td>0.328044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6546846df17f9800</td>\n",
       "      <td>0.495570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3ff0e7a35015be69</td>\n",
       "      <td>0.249899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12ca31f018a2e2b9</td>\n",
       "      <td>0.251461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>03ba802ed4029e4d</td>\n",
       "      <td>0.248569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>c404f8b378cbb008</td>\n",
       "      <td>0.500295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>78243984c02a72e4</td>\n",
       "      <td>0.001503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>de51114bc0faec3e</td>\n",
       "      <td>0.002118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7e3aff857f056bf9</td>\n",
       "      <td>0.004839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>26c3c6dc6174b589</td>\n",
       "      <td>0.007036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b892011ab2e2cabc</td>\n",
       "      <td>0.501691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8247ff562ca185cc</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>c057aecbba832387</td>\n",
       "      <td>0.003739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9f2279ce667b21dc</td>\n",
       "      <td>0.746757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>b9ea2b06a878df6f</td>\n",
       "      <td>0.498702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>79795133c30ef097</td>\n",
       "      <td>0.251459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25522ee5411e63e9</td>\n",
       "      <td>0.388649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.584368\n",
       "1   09e418c93a776564  0.501242\n",
       "2   36baf228038e314b  0.497741\n",
       "3   1f37ead645e7f0c8  0.254475\n",
       "4   71a5b6ad068d531f  0.002114\n",
       "5   474c874d0c07bd21  0.467312\n",
       "6   442c114ed5c4e3c9  0.498677\n",
       "7   b8ae62ea5e1d8bdb  0.002952\n",
       "8   faaddaf8fcba8a3f  0.252589\n",
       "9   ae0262c02566d2ce  0.645575\n",
       "10  a8808e31641e856d  0.248490\n",
       "11  16ae4b99d3601e60  0.253832\n",
       "12  25c555ca3d5a2092  0.748997\n",
       "13  5203a36c501f1b7c  0.996195\n",
       "14  b9fdc772bb8fd61c  0.752162\n",
       "15  7aa5908a77a7ec24  0.250842\n",
       "16  d19ef3979396d47e  0.252857\n",
       "17  fd83613b7843f5e1  0.001611\n",
       "18  2a619016908bfa45  0.547213\n",
       "19  733979d75f59770d  0.328044\n",
       "20  6546846df17f9800  0.495570\n",
       "21  3ff0e7a35015be69  0.249899\n",
       "22  12ca31f018a2e2b9  0.251461\n",
       "23  03ba802ed4029e4d  0.248569\n",
       "24  c404f8b378cbb008  0.500295\n",
       "25  78243984c02a72e4  0.001503\n",
       "26  de51114bc0faec3e  0.002118\n",
       "27  7e3aff857f056bf9  0.004839\n",
       "28  26c3c6dc6174b589  0.007036\n",
       "29  b892011ab2e2cabc  0.501691\n",
       "30  8247ff562ca185cc  0.003050\n",
       "31  c057aecbba832387  0.003739\n",
       "32  9f2279ce667b21dc  0.746757\n",
       "33  b9ea2b06a878df6f  0.498702\n",
       "34  79795133c30ef097  0.251459\n",
       "35  25522ee5411e63e9  0.388649"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['score'] = preds\n",
    "submission.to_csv(\"sberta_submission.csv\", index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MODEL 2 - Using allmini sentence transformer and XGB Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using pretrained Allmini Transformer model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmini_strans = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X_allmini = allmini_strans.encode(sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('all_mini_encode.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentence_pairs, 'embeddings': X_allmini}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Load sentences & embeddings from disc\n",
    "with open('all_mini_encode.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    X_allmini = stored_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_allmini, y, stratify=y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(seed=20)\n",
    "params= {\n",
    "    \"learning_rate\":[0.1], \n",
    "    \"max_depth\":[15]\n",
    "}\n",
    "\n",
    "\n",
    "model = GridSearchCV(model,params, cv=5, n_jobs= -1, verbose = 1)\n",
    "\n",
    "allmini_model=model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(allmini_model, open('allmini_model2.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('allmini_model2.pkl' , 'rb') as f:\n",
    "    lr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04913873149108243"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = lr.predict(X_test)\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = allmini_strans.encode(sentence_pairs_test)\n",
    "preds = lr.predict(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.412687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.448923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.471552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.383177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.330968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.226029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.418601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.225650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.300469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>0.452526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.321143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.386338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.519778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.669634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b9fdc772bb8fd61c</td>\n",
       "      <td>0.718983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7aa5908a77a7ec24</td>\n",
       "      <td>0.390432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d19ef3979396d47e</td>\n",
       "      <td>0.327279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fd83613b7843f5e1</td>\n",
       "      <td>0.293293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2a619016908bfa45</td>\n",
       "      <td>0.460611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>733979d75f59770d</td>\n",
       "      <td>0.373573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6546846df17f9800</td>\n",
       "      <td>0.383131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3ff0e7a35015be69</td>\n",
       "      <td>0.243499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12ca31f018a2e2b9</td>\n",
       "      <td>0.361130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>03ba802ed4029e4d</td>\n",
       "      <td>0.314392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>c404f8b378cbb008</td>\n",
       "      <td>0.453055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>78243984c02a72e4</td>\n",
       "      <td>0.246034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>de51114bc0faec3e</td>\n",
       "      <td>0.160417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7e3aff857f056bf9</td>\n",
       "      <td>0.129728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>26c3c6dc6174b589</td>\n",
       "      <td>0.211064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b892011ab2e2cabc</td>\n",
       "      <td>0.468675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8247ff562ca185cc</td>\n",
       "      <td>0.151583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>c057aecbba832387</td>\n",
       "      <td>0.427947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9f2279ce667b21dc</td>\n",
       "      <td>0.603801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>b9ea2b06a878df6f</td>\n",
       "      <td>0.492679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>79795133c30ef097</td>\n",
       "      <td>0.415511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25522ee5411e63e9</td>\n",
       "      <td>0.263958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.412687\n",
       "1   09e418c93a776564  0.448923\n",
       "2   36baf228038e314b  0.471552\n",
       "3   1f37ead645e7f0c8  0.383177\n",
       "4   71a5b6ad068d531f  0.330968\n",
       "5   474c874d0c07bd21  0.226029\n",
       "6   442c114ed5c4e3c9  0.418601\n",
       "7   b8ae62ea5e1d8bdb  0.225650\n",
       "8   faaddaf8fcba8a3f  0.300469\n",
       "9   ae0262c02566d2ce  0.452526\n",
       "10  a8808e31641e856d  0.321143\n",
       "11  16ae4b99d3601e60  0.386338\n",
       "12  25c555ca3d5a2092  0.519778\n",
       "13  5203a36c501f1b7c  0.669634\n",
       "14  b9fdc772bb8fd61c  0.718983\n",
       "15  7aa5908a77a7ec24  0.390432\n",
       "16  d19ef3979396d47e  0.327279\n",
       "17  fd83613b7843f5e1  0.293293\n",
       "18  2a619016908bfa45  0.460611\n",
       "19  733979d75f59770d  0.373573\n",
       "20  6546846df17f9800  0.383131\n",
       "21  3ff0e7a35015be69  0.243499\n",
       "22  12ca31f018a2e2b9  0.361130\n",
       "23  03ba802ed4029e4d  0.314392\n",
       "24  c404f8b378cbb008  0.453055\n",
       "25  78243984c02a72e4  0.246034\n",
       "26  de51114bc0faec3e  0.160417\n",
       "27  7e3aff857f056bf9  0.129728\n",
       "28  26c3c6dc6174b589  0.211064\n",
       "29  b892011ab2e2cabc  0.468675\n",
       "30  8247ff562ca185cc  0.151583\n",
       "31  c057aecbba832387  0.427947\n",
       "32  9f2279ce667b21dc  0.603801\n",
       "33  b9ea2b06a878df6f  0.492679\n",
       "34  79795133c30ef097  0.415511\n",
       "35  25522ee5411e63e9  0.263958"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_allmini= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/sample_submission.csv')\n",
    "submission_allmini['score'] = preds\n",
    "submission_allmini.to_csv(\"submission_allmini.csv\", index=False)\n",
    "submission_allmini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model 3 - Using glove worlds embeddings and Data Modelling with LSTM***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe is an unsupervised learning algorithm to learn vector representation i.e word embedding for various words. GloVe stands for Global Vectors for Word Representations. In this code I used 100 dimensional GloVe vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/train.csv')\n",
    "df_test= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/test.csv')\n",
    "titles= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/titles.csv')\n",
    "train_df=df_train.merge(titles, left_on='context', right_on='code', how='left')\n",
    "test_df=df_test.merge(titles, left_on='context', right_on='code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text_a\"] = (train_df['anchor']+' '+train_df['title']+' '+train_df['target']).apply(lambda x: lemma_stopwords(x))\n",
    "sentence_pairs = train_df[\"text_a\"]\n",
    "y = train_df['score'].values.astype(np.float64)\n",
    "texts=sentence_pairs\n",
    "labels=y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,Y_train, Y_test = train_test_split(texts, labels, test_size=0.2, random_state = 45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_vec):\n",
    "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "      w_line = line.split()\n",
    "      curr_word = w_line[0]\n",
    "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "  return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map =read_glove_vector (os.path.join(\n",
    "    os.path.expanduser(\"~\"), \"./Documents/Projects/Sciencw/USPatent/glove.6B.100d.txt\"))\n",
    "maxLen = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(words_to_index)\n",
    "embed_vector_len = 250\n",
    "emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "for word, index in words_to_index.items():\n",
    "  embedding_vector = word_to_vec_map.get(word)\n",
    "  if embedding_vector is not None:\n",
    "      embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM stands for Long Term Short Memory and its a type of RNN architecture and is used in NLP problems as it handles long sequence dependencies well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uspatent(input_shape):\n",
    "\n",
    "  X_indices = Input(input_shape)\n",
    "\n",
    "  embeddings = embedding_layer(X_indices)\n",
    "\n",
    "  X = LSTM(128, return_sequences=True)(embeddings)\n",
    "\n",
    "  X = Dropout(0.6)(X)\n",
    "\n",
    "  X = LSTM(128, return_sequences=True)(X)\n",
    "\n",
    "  X = Dropout(0.6)(X)\n",
    "\n",
    "  X = LSTM(128)(X)\n",
    "\n",
    "  X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "  model = Model(inputs=X_indices, outputs=X)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_indices = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "973/973 [==============================] - 500s 514ms/step - loss: 0.0613 - mae: 0.2005\n",
      "Epoch 2/10\n",
      "973/973 [==============================] - 486s 500ms/step - loss: 0.0540 - mae: 0.1829\n",
      "Epoch 3/10\n",
      "973/973 [==============================] - 486s 500ms/step - loss: 0.0457 - mae: 0.1659\n",
      "Epoch 4/10\n",
      "973/973 [==============================] - 493s 506ms/step - loss: 0.0388 - mae: 0.1524\n",
      "Epoch 5/10\n",
      "973/973 [==============================] - 495s 508ms/step - loss: 0.0337 - mae: 0.1416\n",
      "Epoch 6/10\n",
      "973/973 [==============================] - 499s 512ms/step - loss: 0.0295 - mae: 0.1315\n",
      "Epoch 7/10\n",
      "973/973 [==============================] - 491s 505ms/step - loss: 0.0268 - mae: 0.1248\n",
      "Epoch 8/10\n",
      "973/973 [==============================] - 487s 500ms/step - loss: 0.0240 - mae: 0.1173\n",
      "Epoch 9/10\n",
      "973/973 [==============================] - 498s 512ms/step - loss: 0.0220 - mae: 0.1117\n",
      "Epoch 10/10\n",
      "973/973 [==============================] - 486s 499ms/step - loss: 0.0201 - mae: 0.1061\n"
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate = 0.1)\n",
    "model.compile(loss=\"mse\", optimizer=\"Adam\", metrics=[\"mae\"])\n",
    "history=model.fit(X_train_indices, Y_train, batch_size=30, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(history, open('LSTM.pkl', 'wb'))\n",
    "with open('LSTM.pkl' , 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df[\"text_a\"] = (test_df['anchor']+' '+test_df['title']+' '+test_df['target']).apply(lambda x: lemma_stopwords(x))\n",
    "test=test_df[\"text_a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_indices = tokenizer.texts_to_sequences(test)\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "preds = model.predict(X_test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4112d61851461f60</td>\n",
       "      <td>0.570836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09e418c93a776564</td>\n",
       "      <td>0.601534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36baf228038e314b</td>\n",
       "      <td>0.167181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f37ead645e7f0c8</td>\n",
       "      <td>0.268841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71a5b6ad068d531f</td>\n",
       "      <td>0.091575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>474c874d0c07bd21</td>\n",
       "      <td>0.617274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>442c114ed5c4e3c9</td>\n",
       "      <td>0.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8ae62ea5e1d8bdb</td>\n",
       "      <td>0.026787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faaddaf8fcba8a3f</td>\n",
       "      <td>0.253366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ae0262c02566d2ce</td>\n",
       "      <td>0.644704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a8808e31641e856d</td>\n",
       "      <td>0.247214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16ae4b99d3601e60</td>\n",
       "      <td>0.311296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25c555ca3d5a2092</td>\n",
       "      <td>0.842888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5203a36c501f1b7c</td>\n",
       "      <td>0.754458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b9fdc772bb8fd61c</td>\n",
       "      <td>0.771920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7aa5908a77a7ec24</td>\n",
       "      <td>0.361976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>d19ef3979396d47e</td>\n",
       "      <td>0.183335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fd83613b7843f5e1</td>\n",
       "      <td>0.061744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2a619016908bfa45</td>\n",
       "      <td>0.601534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>733979d75f59770d</td>\n",
       "      <td>0.215571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6546846df17f9800</td>\n",
       "      <td>0.643265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3ff0e7a35015be69</td>\n",
       "      <td>0.323825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12ca31f018a2e2b9</td>\n",
       "      <td>0.181596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>03ba802ed4029e4d</td>\n",
       "      <td>0.366426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>c404f8b378cbb008</td>\n",
       "      <td>0.539541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>78243984c02a72e4</td>\n",
       "      <td>0.019858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>de51114bc0faec3e</td>\n",
       "      <td>0.011282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7e3aff857f056bf9</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>26c3c6dc6174b589</td>\n",
       "      <td>0.065474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b892011ab2e2cabc</td>\n",
       "      <td>0.685780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8247ff562ca185cc</td>\n",
       "      <td>0.343905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>c057aecbba832387</td>\n",
       "      <td>0.063044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9f2279ce667b21dc</td>\n",
       "      <td>0.632296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>b9ea2b06a878df6f</td>\n",
       "      <td>0.504316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>79795133c30ef097</td>\n",
       "      <td>0.182871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25522ee5411e63e9</td>\n",
       "      <td>0.389080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     score\n",
       "0   4112d61851461f60  0.570836\n",
       "1   09e418c93a776564  0.601534\n",
       "2   36baf228038e314b  0.167181\n",
       "3   1f37ead645e7f0c8  0.268841\n",
       "4   71a5b6ad068d531f  0.091575\n",
       "5   474c874d0c07bd21  0.617274\n",
       "6   442c114ed5c4e3c9  0.576800\n",
       "7   b8ae62ea5e1d8bdb  0.026787\n",
       "8   faaddaf8fcba8a3f  0.253366\n",
       "9   ae0262c02566d2ce  0.644704\n",
       "10  a8808e31641e856d  0.247214\n",
       "11  16ae4b99d3601e60  0.311296\n",
       "12  25c555ca3d5a2092  0.842888\n",
       "13  5203a36c501f1b7c  0.754458\n",
       "14  b9fdc772bb8fd61c  0.771920\n",
       "15  7aa5908a77a7ec24  0.361976\n",
       "16  d19ef3979396d47e  0.183335\n",
       "17  fd83613b7843f5e1  0.061744\n",
       "18  2a619016908bfa45  0.601534\n",
       "19  733979d75f59770d  0.215571\n",
       "20  6546846df17f9800  0.643265\n",
       "21  3ff0e7a35015be69  0.323825\n",
       "22  12ca31f018a2e2b9  0.181596\n",
       "23  03ba802ed4029e4d  0.366426\n",
       "24  c404f8b378cbb008  0.539541\n",
       "25  78243984c02a72e4  0.019858\n",
       "26  de51114bc0faec3e  0.011282\n",
       "27  7e3aff857f056bf9  0.000491\n",
       "28  26c3c6dc6174b589  0.065474\n",
       "29  b892011ab2e2cabc  0.685780\n",
       "30  8247ff562ca185cc  0.343905\n",
       "31  c057aecbba832387  0.063044\n",
       "32  9f2279ce667b21dc  0.632296\n",
       "33  b9ea2b06a878df6f  0.504316\n",
       "34  79795133c30ef097  0.182871\n",
       "35  25522ee5411e63e9  0.389080"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_lstmglove= pd.read_csv('/Users/jcecilya/Documents/Projects/Sciencw/USPatent/sample_submission.csv')\n",
    "submission_lstmglove['score'] = preds\n",
    "submission_lstmglove.to_csv(\"submission_lstmglove.csv\", index=False)\n",
    "submission_lstmglove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
